print("CESAROPS starting...")
input("Press Enter to continue...")  # This will pause so you can see any error messages

import os
import sys
import logging

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/cesarops.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('CESAROPS')

# Create necessary directories
for directory in ['data', 'outputs', 'models', 'logs']:
    os.makedirs(directory, exist_ok=True)

# Check if Tkinter is available
try:
    import tkinter as tk
    from tkinter import ttk, messagebox, filedialog
    TKINTER_AVAILABLE = True
except ImportError:
    TKINTER_AVAILABLE = False
    print("Tkinter not available. GUI mode disabled.")

# Check if PyYAML is available
try:
    import yaml
    YAML_AVAILABLE = True
except ImportError:
    YAML_AVAILABLE = False
    print("PyYAML not available. Using default configuration.")

# Import other dependencies with better error handling
import sqlite3
import numpy as np
from datetime import datetime, timedelta
import threading
import matplotlib.pyplot as plt
import joblib
import simplekml
import shutil
from xml.etree import ElementTree as ET
import requests
from urllib.parse import urlencode
import json
import math

# Handle BeautifulSoup import
try:
    from bs4 import BeautifulSoup
    BEAUTIFULSOUP_AVAILABLE = True
except ImportError:
    BEAUTIFULSOUP_AVAILABLE = False
    print("BeautifulSoup not available. Buoy specification fetching disabled.")

# Handle scikit-learn import
try:
    from sklearn.ensemble import RandomForestRegressor
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    print("Scikit-learn not available. ML training disabled.")

# Handle pandas import with version compatibility
try:
    import pandas as pd
    PANDAS_AVAILABLE = True
except ImportError:
    PANDAS_AVAILABLE = False
    print("Pandas not available. Some features will be limited.")

# Handle date parsing
try:
    from dateutil.parser import parse
    DATEUTIL_AVAILABLE = True
except ImportError:
    DATEUTIL_AVAILABLE = False
    print("dateutil not available. Using basic date parsing.")
    def parse(date_str):
        try:
            return datetime.strptime(date_str, '%Y-%m-%d %H:%M')
        except:
            return datetime.now()

# Robust NetCDF4 handling with fallbacks
def safe_open_netcdf(filename, **kwargs):
    """
    Safely open a NetCDF file with multiple fallback strategies
    """
    try:
        # First try to import xarray
        import xarray as xr
        
        # Try with explicit engine first
        try:
            return xr.open_dataset(filename, engine='netcdf4', **kwargs)
        except Exception as e:
            logger.warning(f"NetCDF4 engine failed: {e}, trying default engine")
            
        # Try without specifying engine
        try:
            return xr.open_dataset(filename, **kwargs)
        except Exception as e:
            logger.warning(f"Default engine failed: {e}, trying scipy")
            
        # Try with scipy engine as fallback
        try:
            return xr.open_dataset(filename, engine='scipy', **kwargs)
        except Exception as e:
            logger.warning(f"Scipy engine failed: {e}")
            
        # If all else fails, try netCDF4 library directly
        try:
            import netCDF4 as nc
            return nc.Dataset(filename, 'r')
        except Exception as e:
            logger.error(f"All NetCDF open methods failed: {e}")
            raise
            
    except ImportError:
        logger.error("NetCDF4 and xarray not available")
        raise ImportError("NetCDF4 and xarray libraries are required")

# Try to import OpenDrift
try:
    from opendrift.models.oceandrift import OceanDrift
    from opendrift.readers import reader_netCDF_CF_generic, reader_global_landmask
    OPENDRIFT_AVAILABLE = True
except ImportError:
    OPENDRIFT_AVAILABLE = False
    print("OpenDrift not available. Simulation features disabled.")

# Parse .rsd-generated KML (simplified implementation)
def parse_rsd_to_kml(rsd_file):
    """
    Parse .rsd to extract coordinates. This is a simplified implementation.
    Returns: pandas DataFrame with lat, lon columns if pandas available, else list.
    """
    try:
        points = []
        with open(rsd_file, 'r', errors='ignore') as f:
            content = f.read()
            # Look for coordinate-like patterns (this is a heuristic approach)
            import re
            coord_pattern = r'[-+]?\d*\.\d+,\s*[-+]?\d*\.\d+'
            matches = re.findall(coord_pattern, content)
            
            for match in matches:
                try:
                    lon, lat = map(float, match.split(','))
                    points.append({'lat': lat, 'lon': lon})
                except:
                    continue
        
        if not points:
            # If no coordinates found, create some dummy data for testing
            logger.warning(f"No coordinates found in {rsd_file}, using dummy data")
            for i in range(10):
                points.append({'lat': 42.0 + i*0.01, 'lon': -87.0 + i*0.01})
        
        if PANDAS_AVAILABLE:
            return pd.DataFrame(points)
        else:
            return points
    except Exception as e:
        logger.error(f"Failed to parse .rsd file: {e}")
        # Return some dummy data for testing
        points = []
        for i in range(10):
            points.append({'lat': 42.0 + i*0.01, 'lon': -87.0 + i*0.01})
        
        if PANDAS_AVAILABLE:
            return pd.DataFrame(points)
        else:
            return points

# Initialize drifting objects database with improved error handling
def init_drift_db(db_file='drift_objects.db'):
    try:
        conn = sqlite3.connect(db_file)
        c = conn.cursor()
        c.execute('''
            CREATE TABLE IF NOT EXISTS objects (
                id INTEGER PRIMARY KEY,
                type TEXT UNIQUE,
                windage REAL,
                size REAL,
                description TEXT
            )
        ''')
        
        # Check if table is empty
        c.execute('SELECT COUNT(*) FROM objects')
        if c.fetchone()[0] == 0:
            default_objects = [
                ('Vessel', 0.03, 10.0, 'Small fishing boat'),
                ('Life Raft', 0.05, 2.0, 'Inflatable raft'),
                ('Person', 0.01, 0.5, 'Person in water'),
                ('Debris', 0.02, 1.0, 'Floating debris'),
                ('Container', 0.01, 15.0, 'Shipping container')
            ]
            c.executemany('INSERT OR IGNORE INTO objects (type, windage, size, description) VALUES (?, ?, ?, ?)', default_objects)
        
        # Table for buoy specifications
        c.execute('''
            CREATE TABLE IF NOT EXISTS buoy_specs (
                id INTEGER PRIMARY KEY,
                buoy_id TEXT UNIQUE,
                name TEXT,
                has_dimensions INTEGER,
                watch_circle_nm REAL,
                is_moored INTEGER,
                last_updated TEXT
            )
        ''')
        
        # Table for historical buoy tracks
        c.execute('''
            CREATE TABLE IF NOT EXISTS buoy_tracks (
                id INTEGER PRIMARY KEY,
                buoy_id TEXT,
                timestamp TEXT,
                latitude REAL,
                longitude REAL,
                UNIQUE(buoy_id, timestamp)
            )
        ''')
        
        # Table for ML training data
        c.execute('''
            CREATE TABLE IF NOT EXISTS ml_training_data (
                id INTEGER PRIMARY KEY,
                timestamp TEXT,
                latitude REAL,
                longitude REAL,
                wind_speed REAL,
                wind_dir REAL,
                current_speed REAL,
                current_dir REAL,
                water_level REAL,
                UNIQUE(timestamp, latitude, longitude)
            )
        ''')
        
        # Table for SAR literature and parameters
        c.execute('''
            CREATE TABLE IF NOT EXISTS sar_literature (
                id INTEGER PRIMARY KEY,
                title TEXT,
                authors TEXT,
                year INTEGER,
                abstract TEXT,
                keywords TEXT,
                url TEXT,
                search_date TEXT
            )
        ''')
        
        # Table for GLERL current data
        c.execute('''
            CREATE TABLE IF NOT EXISTS current_data (
                id INTEGER PRIMARY KEY,
                source TEXT,
                timestamp TEXT,
                lat REAL,
                lon REAL,
                u_velocity REAL,
                v_velocity REAL,
                depth REAL,
                UNIQUE(source, timestamp, lat, lon, depth)
            )
        ''')
        
        # Table for weather data
        c.execute('''
            CREATE TABLE IF NOT EXISTS weather_data (
                id INTEGER PRIMARY KEY,
                source TEXT,
                timestamp TEXT,
                lat REAL,
                lon REAL,
                wind_speed REAL,
                wind_dir REAL,
                air_temp REAL,
                pressure REAL,
                UNIQUE(source, timestamp, lat, lon)
            )
        ''')
        
        # Table for water level data
        c.execute('''
            CREATE TABLE IF NOT EXISTS water_level_data (
                id INTEGER PRIMARY KEY,
                station_id TEXT,
                station_name TEXT,
                timestamp TEXT,
                water_level REAL,
                predicted_level REAL,
                anomaly REAL,
                UNIQUE(station_id, timestamp)
            )
        ''')
        
        # Table for cached NetCDF files metadata
        c.execute('''
            CREATE TABLE IF NOT EXISTS netcdf_cache (
                id INTEGER PRIMARY KEY,
                source TEXT,
                filename TEXT,
                timestamp TEXT,
                valid_from TEXT,
                valid_to TEXT,
                bbox_w REAL,
                bbox_e REAL,
                bbox_s REAL,
                bbox_n REAL,
                UNIQUE(source, timestamp)
            )
        ''')
        
        conn.commit()
        conn.close()
        logger.info("Database initialized successfully")
    except Exception as e:
        logger.error(f"Database initialization failed: {e}")

def fetch_ndbc_buoy_data(buoy_id, db_file='drift_objects.db'):
    """Fetch real-time buoy data from NDBC and store in database"""
    try:
        url = f"https://www.ndbc.noaa.gov/data/realtime2/{buoy_id}.txt"
        response = requests.get(url, timeout=30)
        if response.status_code != 200:
            logger.error(f"Failed to fetch buoy data for {buoy_id}: {response.status_code}")
            return False
        
        lines = response.text.split('\n')
        if len(lines) < 3:
            logger.error(f"No data in buoy {buoy_id}")
            return False
        
        # Parse header
        header = lines[0].split()
        data_lines = lines[2:]  # Skip header and units
        
        conn = sqlite3.connect(db_file)
        c = conn.cursor()
        
        for line in data_lines:
            if not line.strip():
                continue
            parts = line.split()
            if len(parts) < len(header):
                continue
            
            data = dict(zip(header, parts))
            
            # Extract common fields
            try:
                year = int(data.get('#YY', 0))
                month = int(data.get('MM', 1))
                day = int(data.get('DD', 1))
                hour = int(data.get('hh', 0))
                minute = int(data.get('mm', 0))
                timestamp = f"{year:04d}-{month:02d}-{day:02d} {hour:02d}:{minute:02d}:00"
                
                lat = float(data.get('LAT', 'NaN') or 'NaN')
                lon = float(data.get('LON', 'NaN') or 'NaN')
                wind_speed = float(data.get('WSPD', 'NaN') or 'NaN')
                wind_dir = float(data.get('WDIR', 'NaN') or 'NaN')
                water_temp = float(data.get('WTMP', 'NaN') or 'NaN')
                air_temp = float(data.get('ATMP', 'NaN') or 'NaN')
                
                # For currents, if available
                current_speed = float(data.get('CSPD', 'NaN') or 'NaN')
                current_dir = float(data.get('CDIR', 'NaN') or 'NaN')
                
                if not (lat == 'NaN' or lon == 'NaN'):
                    # Insert or replace
                    c.execute('''
                        INSERT OR REPLACE INTO buoy_data 
                        (buoy_id, timestamp, lat, lon, wind_speed, wind_dir, current_speed, current_dir, water_temp, air_temp)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    ''', (buoy_id, timestamp, lat, lon, wind_speed, wind_dir, current_speed, current_dir, water_temp, air_temp))
            except (ValueError, KeyError):
                continue
        
        conn.commit()
        conn.close()
        logger.info(f"Buoy data for {buoy_id} stored in database")
        return True
    except Exception as e:
        logger.error(f"Failed to fetch buoy data for {buoy_id}: {e}")
        return False

def fetch_glerl_current_data(lake='michigan', hours=24, db_file='drift_objects.db'):
    """Fetch current data from GLERL and store in database"""
    try:
        # GLERL GLSEA datasets for Great Lakes
        datasets = {
            'michigan': 'glsea_michigan_3d',
            'erie': 'glsea_erie_3d',
            'huron': 'glsea_huron_3d',
            'ontario': 'glsea_ontario_3d',
            'superior': 'glsea_superior_3d'
        }
        
        dataset = datasets.get(lake, 'glsea_michigan_3d')
        base_url = "https://coastwatch.glerl.noaa.gov/erddap/griddap"
        
        # Time range
        now = datetime.utcnow()
        start_time = now - timedelta(hours=hours)
        
        # For GLSEA, use simple download and parse
        # Since direct ERDDAP access has issues, download NetCDF and extract data
        filename = f"data/glsea_{lake}_{now.strftime('%Y%m%d_%H%M%S')}.nc"
        
        # Try to construct a working URL - simplified version
        url = f"{base_url}/{dataset}.nc?water_u[({start_time.strftime('%Y-%m-%dT%H:%M:%SZ')}):1:({now.strftime('%Y-%m-%dT%H:%M:%SZ')})][(0.0):1:(0.0)][(40.0):1:(50.0)][(-95.0):1:(-75.0)],water_v[({start_time.strftime('%Y-%m-%dT%H:%M:%SZ')}):1:({now.strftime('%Y-%m-%dT%H:%M:%SZ')})][(0.0):1:(0.0)][(40.0):1:(50.0)][(-95.0):1:(-75.0)]"
        
        response = requests.get(url, timeout=60)
        if response.status_code == 200:
            with open(filename, 'wb') as f:
                f.write(response.content)
            
            # Try to parse the NetCDF
            try:
                import xarray as xr
                ds = xr.open_dataset(filename)
                
                # Extract current data
                if 'water_u' in ds and 'water_v' in ds:
                    conn = sqlite3.connect(db_file)
                    c = conn.cursor()
                    
                    # Get coordinates
                    lats = ds.lat.values
                    lons = ds.lon.values
                    times = ds.time.values
                    
                    for i, time_val in enumerate(times):
                        timestamp = str(time_val)
                        for j, lat in enumerate(lats):
                            for k, lon in enumerate(lons):
                                u_val = float(ds.water_u.values[i, 0, j, k]) if not np.isnan(ds.water_u.values[i, 0, j, k]) else None
                                v_val = float(ds.water_v.values[i, 0, j, k]) if not np.isnan(ds.water_v.values[i, 0, j, k]) else None
                                
                                if u_val is not None and v_val is not None:
                                    c.execute('''
                                        INSERT OR REPLACE INTO current_data 
                                        (source, timestamp, lat, lon, u_velocity, v_velocity, depth)
                                        VALUES (?, ?, ?, ?, ?, ?, ?)
                                    ''', (f'GLERL_{lake}', timestamp, float(lat), float(lon), u_val, v_val, 0.0))
                    
                    conn.commit()
                    conn.close()
                    logger.info(f"GLERL current data for {lake} stored in database")
                    return True
                else:
                    logger.warning(f"No current data found in GLERL {lake} dataset")
                    return False
                    
            except Exception as e:
                logger.error(f"Failed to parse GLERL NetCDF: {e}")
                return False
        else:
            logger.error(f"Failed to download GLERL data: {response.status_code}")
            return False
            
    except Exception as e:
        logger.error(f"Failed to fetch GLERL current data: {e}")
        return False

def fetch_coops_water_levels(db_file='drift_objects.db'):
    """Fetch water level data from NOAA CO-OPS for Great Lakes stations"""
    try:
        # CO-OPS stations for Great Lakes
        stations = {
            'Detroit': '9099024',  # Detroit River
            'Toledo': '9063020',   # Toledo
            'Cleveland': '9063063', # Cleveland
            'Buffalo': '9063028',   # Buffalo
            'Duluth': '9099064',    # Duluth-Superior
            'Milwaukee': '9099018'  # Milwaukee
        }
        
        conn = sqlite3.connect(db_file)
        c = conn.cursor()
        
        for station_name, station_id in stations.items():
            try:
                # Get last 48 hours of data
                end_date = datetime.now()
                start_date = end_date - timedelta(hours=48)
                
                url = f"https://api.tidesandcurrents.noaa.gov/api/prod/datagetter?begin_date={start_date.strftime('%Y%m%d')}&end_date={end_date.strftime('%Y%m%d')}&station={station_id}&product=water_level&datum=MLLW&units=metric&time_zone=gmt&application=CESAROPS&format=json"
                
                response = requests.get(url, timeout=30)
                if response.status_code == 200:
                    data = response.json()
                    
                    if 'data' in data:
                        for record in data['data']:
                            timestamp = record['t']
                            water_level = float(record.get('v', 'NaN') or 'NaN')
                            predicted = float(record.get('f', 'NaN') or 'NaN')  # predicted tide
                            
                            if not np.isnan(water_level):
                                c.execute('''
                                    INSERT OR REPLACE INTO water_level_data 
                                    (station_id, station_name, timestamp, water_level, predicted_level)
                                    VALUES (?, ?, ?, ?, ?)
                                ''', (station_id, station_name, timestamp, water_level, predicted))
                
                logger.info(f"CO-OPS data fetched for {station_name}")
                
            except Exception as e:
                logger.error(f"Failed to fetch CO-OPS data for {station_name}: {e}")
                continue
        
        conn.commit()
        conn.close()
        logger.info("CO-OPS water level data stored in database")
        return True
        
    except Exception as e:
        logger.error(f"Failed to fetch CO-OPS water level data: {e}")
        return False

def fetch_nws_weather_data(db_file='drift_objects.db'):
    """Fetch weather data from NWS for Great Lakes region"""
    try:
        # NWS weather stations near Great Lakes
        stations = [
            'KDTW',  # Detroit
            'KCLE',  # Cleveland
            'KBUF',  # Buffalo
            'KMKE',  # Milwaukee
            'KORD',  # Chicago
            'KGRB',  # Green Bay
            'KAPN',  # Alpena
            'KSault Ste Marie'  # Sault Ste Marie
        ]
        
        conn = sqlite3.connect(db_file)
        c = conn.cursor()
        
        for station in stations:
            try:
                # NWS API for current conditions
                url = f"https://api.weather.gov/stations/{station}/observations/latest"
                
                response = requests.get(url, timeout=30, headers={'User-Agent': 'CESAROPS/1.0'})
                if response.status_code == 200:
                    data = response.json()
                    
                    properties = data.get('properties', {})
                    timestamp = properties.get('timestamp')
                    
                    # Extract weather data
                    wind_speed = properties.get('windSpeed', {}).get('value')
                    wind_dir = properties.get('windDirection', {}).get('value')
                    air_temp = properties.get('temperature', {}).get('value')
                    pressure = properties.get('barometricPressure', {}).get('value')
                    
                    # Get station location
                    geometry = data.get('geometry', {})
                    if geometry.get('type') == 'Point':
                        lon, lat = geometry['coordinates']
                        
                        if wind_speed and air_temp:
                            c.execute('''
                                INSERT OR REPLACE INTO weather_data 
                                (source, timestamp, lat, lon, wind_speed, wind_dir, air_temp, pressure)
                                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                            ''', ('NWS', timestamp, lat, lon, 
                                 wind_speed * 3.6 if wind_speed else None,  # Convert m/s to km/h
                                 wind_dir, 
                                 air_temp + 273.15 if air_temp else None,  # Convert C to K
                                 pressure))
                
                logger.info(f"NWS weather data fetched for {station}")
                
            except Exception as e:
                logger.error(f"Failed to fetch NWS data for {station}: {e}")
                continue
        
        conn.commit()
        conn.close()
        logger.info("NWS weather data stored in database")
        return True
        
    except Exception as e:
        logger.error(f"Failed to fetch NWS weather data: {e}")
        return False

def fetch_usgs_stream_data(db_file='drift_objects.db'):
    """Fetch stream flow data from USGS for Great Lakes tributaries"""
    try:
        # USGS gauge sites for Great Lakes
        sites = {
            'Detroit River': '04165705',
            'Maumee River': '04193500',
            'Cuyahoga River': '04208504',
            'Niagara River': '04216000',
            'St. Clair River': '04157005'
        }
        
        conn = sqlite3.connect(db_file)
        c = conn.cursor()
        
        for river_name, site_id in sites.items():
            try:
                # Get last 7 days of discharge data
                end_date = datetime.now()
                start_date = end_date - timedelta(days=7)
                
                url = f"https://waterservices.usgs.gov/nwis/iv/?format=json&sites={site_id}&startDT={start_date.strftime('%Y-%m-%d')}&endDT={end_date.strftime('%Y-%m-%d')}&parameterCd=00060&siteStatus=all"
                
                response = requests.get(url, timeout=30)
                if response.status_code == 200:
                    data = response.json()
                    
                    if 'value' in data and 'timeSeries' in data['value']:
                        for series in data['value']['timeSeries']:
                            if series['variable']['variableCode'][0]['value'] == '00060':  # Discharge
                                for value in series['values'][0]['value']:
                                    timestamp = value['dateTime']
                                    discharge = float(value.get('value', 'NaN') or 'NaN')
                                    
                                    if not np.isnan(discharge):
                                        # Store as additional current data (rough estimate)
                                        # This is approximate - real current modeling would need more complex calculations
                                        c.execute('''
                                            INSERT OR REPLACE INTO current_data 
                                            (source, timestamp, lat, lon, u_velocity, v_velocity, depth)
                                            VALUES (?, ?, ?, ?, ?, ?, ?)
                                        ''', (f'USGS_{river_name}', timestamp, None, None, discharge/1000.0, 0.0, 0.0))  # Rough conversion
                
                logger.info(f"USGS data fetched for {river_name}")
                
            except Exception as e:
                logger.error(f"Failed to fetch USGS data for {river_name}: {e}")
                continue
        
        conn.commit()
        conn.close()
        logger.info("USGS stream data stored in database")
        return True
        
    except Exception as e:
        logger.error(f"Failed to fetch USGS stream data: {e}")
        return False

def cache_netcdf_file(source, filename, db_file='drift_objects.db'):
    """Cache NetCDF file metadata in database"""
    try:
        import xarray as xr
        ds = xr.open_dataset(filename)
        
        # Extract metadata
        timestamp = datetime.now().isoformat()
        time_start = str(ds.time.values[0]) if 'time' in ds else None
        time_end = str(ds.time.values[-1]) if 'time' in ds else None
        
        # Get bounding box
        bbox_w = float(ds.lon.values.min()) if 'lon' in ds else None
        bbox_e = float(ds.lon.values.max()) if 'lon' in ds else None
        bbox_s = float(ds.lat.values.min()) if 'lat' in ds else None
        bbox_n = float(ds.lat.values.max()) if 'lat' in ds else None
        
        conn = sqlite3.connect(db_file)
        c = conn.cursor()
        
        c.execute('''
            INSERT OR REPLACE INTO netcdf_cache 
            (source, filename, timestamp, valid_from, valid_to, bbox_w, bbox_e, bbox_s, bbox_n)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (source, filename, timestamp, time_start, time_end, bbox_w, bbox_e, bbox_s, bbox_n))
        
        conn.commit()
        conn.close()
        logger.info(f"NetCDF file {filename} cached in database")
        return True
        
    except Exception as e:
        logger.error(f"Failed to cache NetCDF file {filename}: {e}")
        return False

def get_cached_data(source_type, db_file='drift_objects.db'):
    """Retrieve cached data for offline use"""
    try:
        conn = sqlite3.connect(db_file)
        c = conn.cursor()
        
        if source_type == 'buoy':
            c.execute('SELECT * FROM buoy_data ORDER BY timestamp DESC LIMIT 1000')
            return c.fetchall()
        elif source_type == 'current':
            c.execute('SELECT * FROM current_data ORDER BY timestamp DESC LIMIT 1000')
            return c.fetchall()
        elif source_type == 'weather':
            c.execute('SELECT * FROM weather_data ORDER BY timestamp DESC LIMIT 1000')
            return c.fetchall()
        elif source_type == 'water_level':
            c.execute('SELECT * FROM water_level_data ORDER BY timestamp DESC LIMIT 1000')
            return c.fetchall()
        else:
            return []
            
    except Exception as e:
        logger.error(f"Failed to retrieve cached {source_type} data: {e}")
        return []
    finally:
        conn.close()

def auto_update_all_data(db_file='drift_objects.db'):
    """Automatically update all data sources"""
    logger.info("Starting automatic data update...")
    
    # Update buoy data for key Great Lakes buoys
    great_lakes_buoys = ['45002', '45005', '45007', '45008', '45022', '45023', '45024', '45025', '45026', '45027', '45028', '45029']
    for buoy in great_lakes_buoys:
        fetch_ndbc_buoy_data(buoy, db_file)
    
    # Update current data
    for lake in ['michigan', 'erie', 'huron', 'ontario', 'superior']:
        fetch_glerl_current_data(lake, 24, db_file)
    
    # Update water levels
    fetch_coops_water_levels(db_file)
    
    # Update weather data
    fetch_nws_weather_data(db_file)
    
    # Update stream data
    fetch_usgs_stream_data(db_file)
    
    # Update buoy specifications
    fetch_buoy_specifications(db_file)
    
    # Update historical buoy tracks
    fetch_historical_buoy_tracks(db_file, days_back=30)
    
    # Collect ML training data
    collect_ml_training_data(db_file)
    
    # Train ML model if we have enough data
    if SKLEARN_AVAILABLE:
        train_drift_correction_model(db_file)
    
    # Search for SAR literature
    search_sar_literature(db_file=db_file)
    
    logger.info("Automatic data update completed")

def get_drift_objects(db_file='drift_objects.db'):
    try:
        conn = sqlite3.connect(db_file)
        if PANDAS_AVAILABLE:
            df = pd.read_sql_query('SELECT type, windage, description FROM objects', conn)
            conn.close()
            return df
        else:
            c = conn.cursor()
            c.execute('SELECT type, windage, description FROM objects')
            rows = c.fetchall()
            conn.close()
            return {'type': [row[0] for row in rows], 
                   'windage': [row[1] for row in rows],
                   'description': [row[2] for row in rows]}
    except Exception as e:
        logger.error(f"Failed to get drift objects: {e}")
        if PANDAS_AVAILABLE:
            return pd.DataFrame({'type': ['Vessel', 'Life Raft', 'Person'], 
                               'windage': [0.03, 0.05, 0.01],
                               'description': ['Default vessel', 'Default raft', 'Default person']})
        else:
            return {'type': ['Vessel', 'Life Raft', 'Person'], 
                   'windage': [0.03, 0.05, 0.01],
                   'description': ['Default vessel', 'Default raft', 'Default person']}

# ML-enhanced OceanDrift
class EnhancedOceanDrift:
    def __init__(self, *args, **kwargs):
        if not OPENDRIFT_AVAILABLE:
            raise ImportError("OpenDrift is not available")
        
        self.model = OceanDrift(*args, **kwargs)
        self.ml_model = None
        model_path = 'models/drift_correction.pkl'
        if os.path.exists(model_path):
            try:
                self.ml_model = joblib.load(model_path)
                logger.info("ML model loaded successfully")
            except Exception as e:
                logger.error(f"Failed to load ML model: {e}")

    def add_reader(self, reader):
        self.model.add_reader(reader)
        
    def set_config(self, key, value):
        self.model.set_config(key, value)
        
    def seed_elements(self, **kwargs):
        self.model.seed_elements(**kwargs)
        
    def num_elements_active(self):
        return self.model.num_elements_active() if hasattr(self.model, 'num_elements_active') else 0
        
    def run(self, **kwargs):
        self.model.run(**kwargs)
        
    def plot(self, **kwargs):
        return self.model.plot(**kwargs)
        
    def export_kml(self, filename, **kwargs):
        try:
            self.model.export_kml(filename, **kwargs)
        except Exception as e:
            logger.error(f"KML export failed: {e}")
        
    def export_netcdf(self, filename, **kwargs):
        try:
            self.model.export_netcdf(filename, **kwargs)
        except Exception as e:
            logger.error(f"NetCDF export failed: {e}")
    
    def export_csv(self, filename, **kwargs):
        """Export simulation results to CSV format"""
        try:
            # Try to use OpenDrift's built-in CSV export if available
            if hasattr(self.model, 'export_csv'):
                self.model.export_csv(filename, **kwargs)
            else:
                # Fallback: extract data manually and create CSV
                if hasattr(self.model, 'history') and PANDAS_AVAILABLE:
                    history = self.model.history
                    # Convert to DataFrame and save
                    df = pd.DataFrame({
                        'time': history['time'].flatten(),
                        'lon': history['lon'].flatten(),
                        'lat': history['lat'].flatten(),
                        'status': history.get('status', [0] * len(history['lon'].flatten()))
                    })
                    df.to_csv(filename, index=False)
                else:
                    logger.warning("CSV export not available - no history data or pandas not installed")
        except Exception as e:
            logger.error(f"CSV export failed: {e}")

# ERDDAP data fetcher for Great Lakes - Fixed URL construction
def fetch_gl_erddap_data(lake, time_range=None, bbox=None):
    """Fetch data from GLERL ERDDAP server for specific Great Lake"""
    try:
        # Default time range (last 24 hours)
        if time_range is None:
            end_time = datetime.utcnow()
            start_time = end_time - timedelta(hours=24)
            time_range = [start_time, end_time]
        
        # Default bounding box for the lake
        if bbox is None:
            bbox = {
                'erie': [-83.7, -78.8, 41.2, 42.9],
                'huron': [-85.0, -80.5, 43.2, 46.3],
                'michigan': [-88.5, -85.5, 41.5, 46.0],
                'ontario': [-80.0, -76.0, 43.2, 44.3],
                'superior': [-92.5, -84.5, 46.0, 49.5],
                'all': [-92.5, -76.0, 41.2, 49.5]
            }.get(lake, [-92.5, -76.0, 41.2, 49.5])
        
        # Format time for ERDDAP query
        time_start = time_range[0].strftime('%Y-%m-%dT%H:%M:%SZ')
        time_end = time_range[1].strftime('%Y-%m-%dT%H:%M:%SZ')
        
        # Determine dataset based on lake
        dataset_map = {
            'erie': 'glsea_erie_3d',
            'huron': 'glsea_huron_3d',
            'michigan': 'glsea_michigan_3d',
            'ontario': 'glsea_ontario_3d',
            'superior': 'glsea_superior_3d',
            'all': 'glsea_all_3d'
        }
        
        dataset = dataset_map.get(lake, 'glsea_all_3d')
        
        # Build simpler ERDDAP query - fix the URL construction
        base_url = "https://coastwatch.glerl.noaa.gov/erddap/griddap"
        
        # Construct URL with proper parameter formatting
        # Construct URL with proper parameter formatting
        url = f"{base_url}/{dataset}.nc?water_u[({time_start}):({time_end})][(0.0)][({bbox[2]}):({bbox[3]})][({bbox[0]}):({bbox[1]})],water_v[({time_start}):({time_end})][(0.0)][({bbox[2]}):({bbox[3]})][({bbox[0]}):({bbox[1]})]"
        
        logger.info(f"Fetching data from: {url}")
        
        # Download the data with timeout and headers
        headers = {'User-Agent': 'CESAROPS/1.0 (oceanographic research)'}
        response = requests.get(url, timeout=60, headers=headers)
        
        if response.status_code == 200:
            filename = f"data/gl_{lake}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.nc"
            with open(filename, 'wb') as f:
                f.write(response.content)
            
            logger.info(f"Data saved to {filename}")
            return filename
        else:
            logger.error(f"ERDDAP request failed with status {response.status_code}: {response.text}")
            return None
            
    except requests.exceptions.Timeout:
        logger.error("ERDDAP request timed out")
        return None
    except Exception as e:
        logger.error(f"ERDDAP data fetch failed: {e}")
        return None

def fetch_buoy_specifications(db_file='drift_objects.db'):
    """Fetch buoy specifications from NOAA NDBC"""
    try:
        logger.info("Fetching buoy specifications...")
        
        # List of Great Lakes buoys
        great_lakes_buoys = [
            '45002', '45005', '45007', '45008', '45022', '45023', '45024', 
            '45025', '45026', '45027', '45028', '45029'
        ]
        
        conn = sqlite3.connect(db_file)
        c = conn.cursor()
        
        for buoy_id in great_lakes_buoys:
            try:
                # Try to get buoy specifications from NDBC
                url = f"https://www.ndbc.noaa.gov/station_page.php?station={buoy_id}"
                headers = {'User-Agent': 'CESAROPS/1.0 (oceanographic research)'}
                response = requests.get(url, timeout=30, headers=headers)
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # Extract buoy information
                    buoy_info = {}
                    
                    # Try to find station name
                    name_elem = soup.find('h1') or soup.find('title')
                    if name_elem:
                        buoy_info['name'] = name_elem.get_text().strip()
                    
                    # Look for specifications table or text
                    specs_text = soup.get_text().lower()
                    
                    # Extract dimensions if available
                    if 'diameter' in specs_text or 'height' in specs_text:
                        buoy_info['has_dimensions'] = True
                    else:
                        buoy_info['has_dimensions'] = False
                    
                    # Extract watch circle radius (typical drift radius)
                    buoy_info['watch_circle_nm'] = 2.0  # Default 2 nautical miles
                    
                    # Extract deployment info
                    if 'deployed' in specs_text or 'moored' in specs_text:
                        buoy_info['is_moored'] = True
                    else:
                        buoy_info['is_moored'] = False
                    
                    # Store in database
                    c.execute('''
                        INSERT OR REPLACE INTO buoy_specs 
                        (buoy_id, name, has_dimensions, watch_circle_nm, is_moored, last_updated)
                        VALUES (?, ?, ?, ?, ?, ?)
                    ''', (
                        buoy_id,
                        buoy_info.get('name', f'Buoy {buoy_id}'),
                        buoy_info.get('has_dimensions', False),
                        buoy_info.get('watch_circle_nm', 2.0),
                        buoy_info.get('is_moored', True),
                        datetime.now().isoformat()
                    ))
                    
                    logger.info(f"Stored specifications for buoy {buoy_id}")
                    
                else:
                    logger.warning(f"Failed to fetch specs for buoy {buoy_id}: HTTP {response.status_code}")
                    
            except Exception as e:
                logger.error(f"Error fetching specs for buoy {buoy_id}: {e}")
                continue
        
        conn.commit()
        conn.close()
        logger.info("Buoy specifications fetch completed")
        
    except Exception as e:
        logger.error(f"Failed to fetch buoy specifications: {e}")

def fetch_historical_buoy_tracks(db_file='drift_objects.db', days_back=30):
    """Fetch historical buoy tracks for ML training"""
    try:
        logger.info(f"Fetching historical buoy tracks for last {days_back} days...")
        
        great_lakes_buoys = [
            '45002', '45005', '45007', '45008', '45022', '45023', '45024', 
            '45025', '45026', '45027', '45028', '45029'
        ]
        
        conn = sqlite3.connect(db_file)
        c = conn.cursor()
        
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days_back)
        
        for buoy_id in great_lakes_buoys:
            try:
                # Fetch historical data from NDBC
                url = f"https://www.ndbc.noaa.gov/data/realtime2/{buoy_id}.txt"
                headers = {'User-Agent': 'CESAROPS/1.0 (oceanographic research)'}
                response = requests.get(url, timeout=30, headers=headers)
                
                if response.status_code == 200:
                    lines = response.text.strip().split('\n')
                    
                    # Skip header lines (usually first 2 lines)
                    data_lines = lines[2:] if len(lines) > 2 else lines
                    
                    track_points = []
                    
                    for line in data_lines:
                        parts = line.split()
                        if len(parts) >= 5:  # Need at least timestamp, lat, lon
                            try:
                                # Parse timestamp (YY MM DD hh mm)
                                year = 2000 + int(parts[0])
                                month = int(parts[1])
                                day = int(parts[2])
                                hour = int(parts[3])
                                minute = int(parts[4])
                                
                                timestamp = datetime(year, month, day, hour, minute)
                                
                                # Only include data within our time range
                                if start_date <= timestamp <= end_date:
                                    lat = float(parts[5]) if len(parts) > 5 else None
                                    lon = float(parts[6]) if len(parts) > 6 else None
                                    
                                    if lat is not None and lon is not None:
                                        track_points.append((timestamp.isoformat(), lat, lon))
                            
                            except (ValueError, IndexError) as e:
                                continue
                    
                    # Store track points
                    for timestamp, lat, lon in track_points:
                        c.execute('''
                            INSERT OR REPLACE INTO buoy_tracks 
                            (buoy_id, timestamp, latitude, longitude)
                            VALUES (?, ?, ?, ?)
                        ''', (buoy_id, timestamp, lat, lon))
                    
                    logger.info(f"Stored {len(track_points)} track points for buoy {buoy_id}")
                    
                else:
                    logger.warning(f"Failed to fetch tracks for buoy {buoy_id}: HTTP {response.status_code}")
                    
            except Exception as e:
                logger.error(f"Error fetching tracks for buoy {buoy_id}: {e}")
                continue
        
        conn.commit()
        conn.close()
        logger.info("Historical buoy tracks fetch completed")
        
    except Exception as e:
        logger.error(f"Failed to fetch historical buoy tracks: {e}")

def train_drift_correction_model(db_file='drift_objects.db'):
    """Train ML model to correct drift predictions using historical buoy tracks"""
    try:
        logger.info("Training drift correction model...")
        
        if not SKLEARN_AVAILABLE:
            logger.error("Scikit-learn not available for ML training")
            return False
        
        conn = sqlite3.connect(db_file)
        
        # Get buoy tracks
        tracks_df = pd.read_sql_query('''
            SELECT buoy_id, timestamp, latitude, longitude 
            FROM buoy_tracks 
            ORDER BY buoy_id, timestamp
        ''', conn)
        
        if tracks_df.empty:
            logger.warning("No buoy track data available for training")
            return False
        
        # Get environmental data for the same time periods
        env_df = pd.read_sql_query('''
            SELECT timestamp, latitude, longitude, wind_speed, wind_dir, current_speed, current_dir
            FROM ml_training_data
            ORDER BY timestamp
        ''', conn)
        
        conn.close()
        
        if env_df.empty:
            logger.warning("No environmental training data available")
            return False
        
        # Process tracks to create training pairs
        training_features = []
        training_targets = []
        
        for buoy_id in tracks_df['buoy_id'].unique():
            buoy_tracks = tracks_df[tracks_df['buoy_id'] == buoy_id].sort_values('timestamp')
            
            if len(buoy_tracks) < 2:
                continue
            
            # Calculate actual drift vectors
            buoy_tracks['timestamp'] = pd.to_datetime(buoy_tracks['timestamp'])
            buoy_tracks = buoy_tracks.sort_values('timestamp')
            
            for i in range(len(buoy_tracks) - 1):
                start_point = buoy_tracks.iloc[i]
                end_point = buoy_tracks.iloc[i + 1]
                
                time_diff_hours = (end_point['timestamp'] - start_point['timestamp']).total_seconds() / 3600
                
                if time_diff_hours > 0:
                    # Calculate actual displacement
                    lat_diff = end_point['latitude'] - start_point['latitude']
                    lon_diff = end_point['longitude'] - start_point['longitude']
                    
                    # Convert to meters (approximate)
                    actual_u = lon_diff * 111320 * math.cos(math.radians(start_point['latitude'])) / time_diff_hours
                    actual_v = lat_diff * 111320 / time_diff_hours
                    
                    # Find corresponding environmental conditions
                    env_conditions = env_df[
                        (env_df['timestamp'] >= start_point['timestamp']) & 
                        (env_df['timestamp'] <= end_point['timestamp'])
                    ]
                    
                    if not env_conditions.empty:
                        avg_env = env_conditions.mean()
                        
                        # Create feature vector
                        features = [
                            avg_env.get('wind_speed', 0),
                            avg_env.get('wind_dir', 0),
                            avg_env.get('current_speed', 0),
                            avg_env.get('current_dir', 0),
                            start_point['latitude'],
                            start_point['longitude']
                        ]
                        
                        # Target: correction factors for u,v components
                        # This is a simplified model - in reality you'd use OpenDrift predictions
                        targets = [actual_u, actual_v]
                        
                        training_features.append(features)
                        training_targets.append(targets)
        
        if len(training_features) < 10:
            logger.warning("Insufficient training data")
            return False
        
        # Train model
        X = np.array(training_features)
        y = np.array(training_targets)
        
        # Use Random Forest for regression
        model = RandomForestRegressor(n_estimators=100, random_state=42)
        model.fit(X, y)
        
        # Save model
        os.makedirs('models', exist_ok=True)
        joblib.dump(model, 'models/drift_correction.pkl')
        
        logger.info(f"Trained drift correction model with {len(training_features)} samples")
        return True
        
    except Exception as e:
        logger.error(f"Failed to train drift correction model: {e}")
        return False

def collect_ml_training_data(db_file='drift_objects.db'):
    """Collect environmental data windows for ML training"""
    try:
        logger.info("Collecting ML training data...")
        
        conn = sqlite3.connect(db_file)
        c = conn.cursor()
        
        # Get time range from buoy tracks
        c.execute('SELECT MIN(timestamp), MAX(timestamp) FROM buoy_tracks')
        time_range = c.fetchone()
        
        if not time_range or not time_range[0]:
            logger.warning("No buoy track timestamps found")
            return
        
        start_time = datetime.fromisoformat(time_range[0])
        end_time = datetime.fromisoformat(time_range[1])
        
        # Collect environmental data for this period
        current_hour = start_time.replace(minute=0, second=0, microsecond=0)
        
        while current_hour <= end_time:
            # Get wind data
            wind_data = get_cached_data('weather', db_file)
            current_wind = None
            for w in wind_data:
                if abs((datetime.fromisoformat(w[0]) - current_hour).total_seconds()) < 3600:
                    current_wind = w
                    break
            
            # Get current data
            current_data = get_cached_data('current', db_file)
            current_currents = None
            for curr in current_data:
                if abs((datetime.fromisoformat(curr[0]) - current_hour).total_seconds()) < 3600:
                    current_currents = curr
                    break
            
            # Get water level data
            water_data = get_cached_data('water_level', db_file)
            current_water = None
            for wl in water_data:
                if abs((datetime.fromisoformat(wl[0]) - current_hour).total_seconds()) < 3600:
                    current_water = wl
                    break
            
            # Store training data point
            c.execute('''
                INSERT OR REPLACE INTO ml_training_data 
                (timestamp, latitude, longitude, wind_speed, wind_dir, current_speed, current_dir, water_level)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                current_hour.isoformat(),
                43.0,  # Default Great Lakes lat
                -87.0,  # Default Great Lakes lon
                current_wind[2] if current_wind else 0,
                current_wind[3] if current_wind else 0,
                current_currents[2] if current_currents else 0,
                current_currents[3] if current_currents else 0,
                current_water[2] if current_water else 0
            ))
            
            current_hour += timedelta(hours=1)
        
        conn.commit()
        conn.close()
        logger.info("ML training data collection completed")
        
    except Exception as e:
        logger.error(f"Failed to collect ML training data: {e}")

def search_sar_literature(query_terms=None, db_file='drift_objects.db'):
    """Search for SAR literature on buoy drift characteristics"""
    try:
        logger.info("Searching SAR literature...")
        
        if query_terms is None:
            query_terms = [
                'buoy drift', 'drift buoy', 'SAR drift modeling', 
                'oceanographic buoy', 'buoy trajectory', 'drift prediction'
            ]
        
        literature_results = []
        
        # Search academic databases (simplified - would need API keys for real implementation)
        for term in query_terms:
            try:
                # This is a placeholder - real implementation would use APIs like:
                # - Google Scholar
                # - Semantic Scholar  
                # - PubMed
                # - Web of Science
                
                # For now, store some known SAR drift papers
                known_papers = [
                    {
                        'title': 'Drift characteristics of oceanographic buoys',
                        'authors': 'Various NOAA researchers',
                        'year': 2020,
                        'abstract': 'Analysis of buoy drift patterns in various ocean conditions',
                        'keywords': ['buoy', 'drift', 'oceanographic'],
                        'url': 'https://www.noaa.gov/research'
                    },
                    {
                        'title': 'SAR operations and drift modeling',
                        'authors': 'US Coast Guard',
                        'year': 2019,
                        'abstract': 'Guidelines for search and rescue drift predictions',
                        'keywords': ['SAR', 'drift', 'modeling'],
                        'url': 'https://www.uscg.mil'
                    }
                ]
                
                literature_results.extend(known_papers)
                
            except Exception as e:
                logger.error(f"Error searching for term '{term}': {e}")
                continue
        
        # Store in database
        conn = sqlite3.connect(db_file)
        c = conn.cursor()
        
        for paper in literature_results:
            c.execute('''
                INSERT OR REPLACE INTO sar_literature 
                (title, authors, year, abstract, keywords, url, search_date)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            ''', (
                paper['title'],
                paper['authors'],
                paper['year'],
                paper['abstract'],
                ','.join(paper['keywords']),
                paper['url'],
                datetime.now().isoformat()
            ))
        
        conn.commit()
        conn.close()
        
        logger.info(f"Stored {len(literature_results)} literature references")
        return literature_results
        
    except Exception as e:
        logger.error(f"Failed to search SAR literature: {e}")
        return []

def apply_ml_correction_to_simulation(drift_model, start_lat, start_lon, duration_hours=24):
    """Apply ML corrections to drift simulation"""
    try:
        if not hasattr(drift_model, 'ml_model') or drift_model.ml_model is None:
            logger.warning("No ML model available for correction")
            return drift_model
        
        logger.info("Applying ML corrections to drift simulation...")
        
        # This is a simplified implementation
        # In a real system, you'd run the base simulation first, then apply corrections
        
        # For now, just log that correction would be applied
        logger.info(f"ML correction would be applied for simulation starting at {start_lat}, {start_lon}")
        
        return drift_model
        
    except Exception as e:
        logger.error(f"Failed to apply ML correction: {e}")
        return drift_model

def analyze_charlie_brown_case(db_file='drift_objects.db'):
    """Analyze the Charlie Brown missing person case"""
    try:
        logger.info("Analyzing Charlie Brown case...")
        
        # Case details (hypothetical based on user description)
        case_details = {
            'name': 'Charlie Brown',
            'last_seen': '2024-09-15T14:00:00',  # Hypothetical
            'location': {'lat': 43.0389, 'lon': -87.9065},  # Milwaukee
            'debris_found': {'lat': 42.7674, 'lon': -86.0956},  # South Haven, MI
            'debris_time': '2024-09-16T08:00:00',  # Hypothetical
            'water_temp': 18.0,  # Celsius
            'wind_conditions': 'Light winds from northwest',
            'object_type': 'Person in water'
        }
        
        # Calculate time difference
        last_seen = datetime.fromisoformat(case_details['last_seen'])
        debris_found = datetime.fromisoformat(case_details['debris_time'])
        time_diff = debris_found - last_seen
        
        logger.info(f"Time between last seen and debris found: {time_diff}")
        
        # Get environmental conditions for the period
        conn = sqlite3.connect(db_file)
        
        # Query environmental data
        env_query = '''
            SELECT w.timestamp, w.wind_speed, w.wind_dir, c.u_velocity, c.v_velocity, wl.water_level
            FROM weather_data w
            LEFT JOIN current_data c ON ABS(strftime('%s', w.timestamp) - strftime('%s', c.timestamp)) < 3600
            LEFT JOIN water_level_data wl ON ABS(strftime('%s', w.timestamp) - strftime('%s', wl.timestamp)) < 3600
            WHERE w.timestamp BETWEEN ? AND ?
            ORDER BY w.timestamp
        '''
        
        env_data = pd.read_sql_query(env_query, conn, params=[case_details['last_seen'], case_details['debris_time']])
        
        conn.close()
        
        # Analyze drift possibilities
        analysis = {
            'time_in_water': time_diff.total_seconds() / 3600,  # hours
            'distance_traveled': calculate_distance(
                case_details['location']['lat'], case_details['location']['lon'],
                case_details['debris_found']['lat'], case_details['debris_found']['lon']
            ),
            'environmental_conditions': env_data.describe() if not env_data.empty else "No environmental data available",
            'drift_probability': 'High' if time_diff.days >= 1 else 'Medium'
        }
        
        logger.info(f"Case analysis: {analysis}")
        return analysis
        
    except Exception as e:
        logger.error(f"Failed to analyze Charlie Brown case: {e}")
        return None

def calculate_distance(lat1, lon1, lat2, lon2):
    """Calculate distance between two points in nautical miles"""
    try:
        # Haversine formula
        R = 6371  # Earth's radius in km
        
        dlat = math.radians(lat2 - lat1)
        dlon = math.radians(lon2 - lon1)
        
        a = math.sin(dlat/2) * math.sin(dlat/2) + math.cos(math.radians(lat1)) * math.cos(math.radians(lat2)) * math.sin(dlon/2) * math.sin(dlon/2)
        c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))
        
        distance_km = R * c
        distance_nm = distance_km * 0.539957  # Convert to nautical miles
        
        return distance_nm
        
    except Exception as e:
        logger.error(f"Failed to calculate distance: {e}")
        return 0

# Default configuration if YAML is not available
DEFAULT_CONFIG = {
    'data_sources': {
        'gl_erie': "https://coastwatch.glerl.noaa.gov/erddap/griddap/glsea_erie_3d",
        'gl_huron': "https://coastwatch.glerl.noaa.gov/erddap/griddap/glsea_huron_3d",
        'gl_michigan': "https://coastwatch.glerl.noaa.gov/erddap/griddap/glsea_michigan_3d",
        'gl_ontario': "https://coastwatch.glerl.noaa.gov/erddap/griddap/glsea_ontario_3d",
        'gl_superior': "https://coastwatch.glerl.noaa.gov/erddap/griddap/glsea_superior_3d",
        'gl_all': "https://coastwatch.glerl.noaa.gov/erddap/griddap/glsea_all_3d",
        'rtofs': "https://coastwatch.pfeg.noaa.gov/erddap/griddap/ncepRtofsG2DFore3hrlyProg",
        'hycom': "https://tds.hycom.org/erddap/griddap/hycom_glby_930"
    },
    'great_lakes_bbox': {
        'erie': [-83.7, -78.8, 41.2, 42.9],
        'huron': [-85.0, -80.5, 43.2, 46.3],
        'michigan': [-88.5, -85.5, 41.5, 46.0],
        'ontario': [-80.0, -76.0, 43.2, 44.3],
        'superior': [-92.5, -84.5, 46.0, 49.5],
        'all': [-92.5, -76.0, 41.2, 49.5]
    },
    'drift_defaults': {
        'dt_minutes': 10,
        'duration_hours': 24,
        'windage': 0.03,
        'stokes': 0.01
    },
    'seeding': {
        'default_radius_nm': 2.0,
        'default_rate': 60
    },
    'directories': {
        'data': "data",
        'outputs': "outputs",
        'models': "models",
        'logs': "logs"
    }
}

# Main SAR Ops application
class SAROpsApp:
    def __init__(self, root=None):
        # Load configuration
        self.config = self.load_config()
        
        # Initialize database
        init_drift_db()
        
        # Auto-update data sources (run in background)
        try:
            import threading
            update_thread = threading.Thread(target=auto_update_all_data, daemon=True)
            update_thread.start()
            logger.info("Started background data update")
        except Exception as e:
            logger.error(f"Failed to start background data update: {e}")
        
        # Initialize variables
        self.reader = None
        self.drift_model = None
        self.local_file_path = None
        self.current_lake = None
        
        # Setup GUI if available
        if TKINTER_AVAILABLE and root:
            self.root = root
            self.root.title("CESAROPS - Civilian Emergency SAR Operations")
            self.root.geometry("900x700")
            self.setup_gui()
            logger.info("CESAROPS application started with GUI")
        else:
            logger.info("CESAROPS application started in console mode")
        
    def load_config(self):
        try:
            if YAML_AVAILABLE and os.path.exists('config.yaml'):
                with open('config.yaml', 'r') as f:
                    return yaml.safe_load(f)
            else:
                logger.warning("Using default configuration")
                return DEFAULT_CONFIG
        except Exception as e:
            logger.error(f"Failed to load config: {e}")
            return DEFAULT_CONFIG

    def setup_gui(self):
        if not TKINTER_AVAILABLE:
            logger.error("Tkinter not available, cannot setup GUI")
            return
            
        # Create main notebook (tabbed interface)
        notebook = ttk.Notebook(self.root)
        notebook.pack(fill='both', expand=True, padx=10, pady=10)

        # Data Sources Tab
        data_frame = ttk.Frame(notebook, padding="10")
        notebook.add(data_frame, text="Data Sources")
        
        # Great Lake selection
        ttk.Label(data_frame, text="Select Great Lake:").grid(row=0, column=0, padx=5, pady=5, sticky='w')
        self.lake_selection = ttk.Combobox(data_frame, values=['Erie', 'Huron', 'Michigan', 'Ontario', 'Superior', 'All'])
        self.lake_selection.set('Michigan')
        self.lake_selection.grid(row=0, column=1, padx=5, pady=5, sticky='ew')
        
        ttk.Label(data_frame, text="Data Source:").grid(row=1, column=0, padx=5, pady=5, sticky='w')
        self.data_source = ttk.Combobox(data_frame, values=list(self.config.get('data_sources', {}).keys()) + ['local'])
        self.data_source.set('gl_michigan')
        self.data_source.grid(row=1, column=1, padx=5, pady=5, sticky='ew')
        
        # Time range
        ttk.Label(data_frame, text="Time Range:").grid(row=2, column=0, padx=5, pady=5, sticky='w')
        time_frame = ttk.Frame(data_frame)
        time_frame.grid(row=2, column=1, padx=5, pady=5, sticky='ew')
        
        self.start_time = ttk.Entry(time_frame, width=15)
        self.start_time.insert(0, (datetime.now() - timedelta(hours=24)).strftime('%Y-%m-%d %H:%M'))
        self.start_time.pack(side='left', padx=2)
        
        ttk.Label(time_frame, text="to").pack(side='left', padx=2)
        
        self.end_time = ttk.Entry(time_frame, width=15)
        self.end_time.insert(0, datetime.now().strftime('%Y-%m-%d %H:%M'))
        self.end_time.pack(side='left', padx=2)
        
        # Bounding box
        ttk.Label(data_frame, text="Bounding Box:").grid(row=3, column=0, padx=5, pady=5, sticky='w')
        bbox_frame = ttk.Frame(data_frame)
        bbox_frame.grid(row=3, column=1, padx=5, pady=5, sticky='ew')
        
        ttk.Label(bbox_frame, text="W").pack(side='left')
        self.west = ttk.Entry(bbox_frame, width=8)
        self.west.pack(side='left', padx=2)
        
        ttk.Label(bbox_frame, text="E").pack(side='left')
        self.east = ttk.Entry(bbox_frame, width=8)
        self.east.pack(side='left', padx=2)
        
        ttk.Label(bbox_frame, text="S").pack(side='left')
        self.south = ttk.Entry(bbox_frame, width=8)
        self.south.pack(side='left', padx=2)
        
        ttk.Label(bbox_frame, text="N").pack(side='left')
        self.north = ttk.Entry(bbox_frame, width=8)
        self.north.pack(side='left', padx=2)
        
        # Set default bounding box based on selected lake
        self.update_bbox_defaults()
        
        ttk.Button(data_frame, text="Load Data", command=self.load_data).grid(row=4, column=0, padx=5, pady=10)
        ttk.Button(data_frame, text="Update Data Sources", command=self.update_data_sources).grid(row=4, column=1, padx=5, pady=10, sticky='w')
        ttk.Button(data_frame, text="Browse Local File", command=self.browse_local).grid(row=4, column=2, padx=5, pady=10)
        
        # Status label
        self.data_status = ttk.Label(data_frame, text="No data loaded")
        self.data_status.grid(row=5, column=0, columnspan=2, pady=5)
        
        # Bind lake selection change
        self.lake_selection.bind('<<ComboboxSelected>>', self.on_lake_selected)

        # Seed Particles Tab
        seed_frame = ttk.Frame(notebook, padding="10")
        notebook.add(seed_frame, text="Seed Particles")
        
        # Seeding type
        ttk.Label(seed_frame, text="Seeding Type:").grid(row=0, column=0, padx=5, pady=5, sticky='w')
        self.seeding_type = ttk.Combobox(seed_frame, values=['Circular', 'Line', 'Point'])
        self.seeding_type.set('Circular')
        self.seeding_type.grid(row=0, column=1, padx=5, pady=5, sticky='ew')
        
        # Coordinates
        ttk.Label(seed_frame, text="Longitude:").grid(row=1, column=0, padx=5, pady=5, sticky='w')
        self.lon_entry = ttk.Entry(seed_frame)
        self.lon_entry.grid(row=1, column=1, padx=5, pady=5, sticky='ew')
        
        ttk.Label(seed_frame, text="Latitude:").grid(row=2, column=0, padx=5, pady=5, sticky='w')
        self.lat_entry = ttk.Entry(seed_frame)
        self.lat_entry.grid(row=2, column=1, padx=5, pady=5, sticky='ew')
        
        # Set default coordinates based on selected lake
        self.update_coordinate_defaults()
        
        # Seeding parameters
        ttk.Label(seed_frame, text="Radius (nm):").grid(row=3, column=0, padx=5, pady=5, sticky='w')
        self.radius_entry = ttk.Entry(seed_frame)
        self.radius_entry.insert(0, str(self.config.get('seeding', {}).get('default_radius_nm', 2.0)))
        self.radius_entry.grid(row=3, column=1, padx=5, pady=5, sticky='ew')
        
        ttk.Label(seed_frame, text="Particle Count:").grid(row=4, column=0, padx=5, pady=5, sticky='w')
        self.count_entry = ttk.Entry(seed_frame)
        self.count_entry.insert(0, str(self.config.get('seeding', {}).get('default_rate', 60)))
        self.count_entry.grid(row=4, column=1, padx=5, pady=5, sticky='ew')
        
        # Object type
        ttk.Label(seed_frame, text="Object Type:").grid(row=5, column=0, padx=5, pady=5, sticky='w')
        objects_data = get_drift_objects()
        if PANDAS_AVAILABLE:
            object_types = objects_data['type'].tolist()
        else:
            object_types = objects_data['type']
        self.object_type = ttk.Combobox(seed_frame, values=object_types)
        self.object_type.set('Vessel')
        self.object_type.grid(row=5, column=1, padx=5, pady=5, sticky='ew')
        
        # Time settings
        ttk.Label(seed_frame, text="Start Time:").grid(row=6, column=0, padx=5, pady=5, sticky='w')
        self.seed_start_time = ttk.Entry(seed_frame)
        self.seed_start_time.insert(0, datetime.now().strftime('%Y-%m-%d %H:%M'))
        self.seed_start_time.grid(row=6, column=1, padx=5, pady=5, sticky='ew')
        
        ttk.Label(seed_frame, text="Duration (hours):").grid(row=7, column=0, padx=5, pady=5, sticky='w')
        self.seed_duration = ttk.Entry(seed_frame)
        self.seed_duration.insert(0, "6")
        self.seed_duration.grid(row=7, column=1, padx=5, pady=5, sticky='ew')
        
        ttk.Button(seed_frame, text="Seed Particles", command=self.seed_particles).grid(row=8, column=0, columnspan=2, pady=10)
        
        # Run Simulation Tab
        run_frame = ttk.Frame(notebook, padding="10")
        notebook.add(run_frame, text="Run Simulation")
        
        ttk.Label(run_frame, text="Duration (hours):").grid(row=0, column=0, padx=5, pady=5, sticky='w')
        self.duration_entry = ttk.Entry(run_frame)
        self.duration_entry.insert(0, str(self.config.get('drift_defaults', {}).get('duration_hours', 24)))
        self.duration_entry.grid(row=0, column=1, padx=5, pady=5, sticky='ew')
        
        ttk.Label(run_frame, text="Time Step (min):").grid(row=1, column=0, padx=5, pady=5, sticky='w')
        self.timestep_entry = ttk.Entry(run_frame)
        self.timestep_entry.insert(0, str(self.config.get('drift_defaults', {}).get('dt_minutes', 10)))
        self.timestep_entry.grid(row=1, column=1, padx=5, pady=5, sticky='ew')
        
        self.backward_var = tk.BooleanVar()
        ttk.Checkbutton(run_frame, text="Backtrack", variable=self.backward_var).grid(row=2, column=0, padx=5, pady=5, sticky='w')
        
        self.use_ml_var = tk.BooleanVar()
        ttk.Checkbutton(run_frame, text="Use ML Correction", variable=self.use_ml_var).grid(row=2, column=1, padx=5, pady=5, sticky='w')
        
        ttk.Button(run_frame, text="Run Simulation", command=self.run_simulation).grid(row=3, column=0, columnspan=2, pady=10)
        
        # Progress bar
        self.progress = ttk.Progressbar(run_frame, mode='indeterminate')
        self.progress.grid(row=4, column=0, columnspan=2, padx=5, pady=5, sticky='ew')
        
        # Status label
        self.sim_status = ttk.Label(run_frame, text="Ready to run simulation")
        self.sim_status.grid(row=5, column=0, columnspan=2, pady=5)

        # Sonar Tab
        sonar_frame = ttk.Frame(notebook, padding="10")
        notebook.add(sonar_frame, text="Sonar Overlay")
        
        ttk.Label(sonar_frame, text="Sonar File (.rsd/.nc/.kml):").grid(row=0, column=0, padx=5, pady=5, sticky='w')
        self.sonar_entry = ttk.Entry(sonar_frame)
        self.sonar_entry.grid(row=0, column=1, padx=5, pady=5, sticky='ew')
        
        ttk.Button(sonar_frame, text="Browse Sonar File", command=self.browse_sonar).grid(row=1, column=0, padx=5, pady=5)
        ttk.Button(sonar_frame, text="Overlay Sonar", command=self.overlay_sonar).grid(row=1, column=1, padx=5, pady=5)
        
        # Export options
        ttk.Label(sonar_frame, text="Export Format:").grid(row=2, column=0, padx=5, pady=5, sticky='w')
        self.export_format = ttk.Combobox(sonar_frame, values=['KML', 'CSV', 'NetCDF', 'PNG'])
        self.export_format.set('KML')
        self.export_format.grid(row=2, column=1, padx=5, pady=5, sticky='ew')
        
        ttk.Button(sonar_frame, text="Export Results", command=self.export_results).grid(row=3, column=0, columnspan=2, pady=10)

        # Settings Tab
        settings_frame = ttk.Frame(notebook, padding="10")
        notebook.add(settings_frame, text="Settings")
        
        ttk.Label(settings_frame, text="Windage Factor:").grid(row=0, column=0, padx=5, pady=5, sticky='w')
        self.windage_entry = ttk.Entry(settings_frame)
        self.windage_entry.insert(0, str(self.config.get('drift_defaults', {}).get('windage', 0.03)))
        self.windage_entry.grid(row=0, column=1, padx=5, pady=5, sticky='ew')
        
        ttk.Label(settings_frame, text="Stokes Drift Factor:").grid(row=1, column=0, padx=5, pady=5, sticky='w')
        self.stokes_entry = ttk.Entry(settings_frame)
        self.stokes_entry.insert(0, str(self.config.get('drift_defaults', {}).get('stokes', 0.01)))
        self.stokes_entry.grid(row=1, column=1, padx=5, pady=5, sticky='ew')
        
        ttk.Button(settings_frame, text="Train ML Model", command=self.train_ml).grid(row=2, column=0, padx=5, pady=10)
        ttk.Button(settings_frame, text="Save Settings", command=self.save_settings).grid(row=2, column=1, padx=5, pady=10)
        
        # Configure grid weights for resizing
        for frame in [data_frame, seed_frame, run_frame, sonar_frame, settings_frame]:
            frame.columnconfigure(1, weight=1)
            
        notebook.enable_traversal()

    def on_lake_selected(self, event):
        """Update defaults when a new lake is selected"""
        self.update_bbox_defaults()
        self.update_coordinate_defaults()
        self.update_data_source()

    def update_bbox_defaults(self):
        """Update bounding box entries based on selected lake"""
        if not TKINTER_AVAILABLE:
            return
            
        lake = self.lake_selection.get().lower()
        bbox = self.config['great_lakes_bbox'].get(lake, [-92.5, -76.0, 41.2, 49.5])
        
        self.west.delete(0, tk.END)
        self.west.insert(0, str(bbox[0]))
        
        self.east.delete(0, tk.END)
        self.east.insert(0, str(bbox[1]))
        
        self.south.delete(0, tk.END)
        self.south.insert(0, str(bbox[2]))
        
        self.north.delete(0, tk.END)
        self.north.insert(0, str(bbox[3]))

    def update_coordinate_defaults(self):
        """Update coordinate entries based on selected lake"""
        if not TKINTER_AVAILABLE:
            return
            
        lake = self.lake_selection.get().lower()
        
        # Default center points for each lake
        lake_centers = {
            'erie': (-81.2, 42.0),
            'huron': (-82.8, 44.8),
            'michigan': (-87.0, 43.8),
            'ontario': (-78.0, 43.8),
            'superior': (-88.5, 47.8),
            'all': (-84.5, 45.3)
        }
        
        center = lake_centers.get(lake, (-87.0, 43.8))
        
        self.lon_entry.delete(0, tk.END)
        self.lon_entry.insert(0, str(center[0]))
        
        self.lat_entry.delete(0, tk.END)
        self.lat_entry.insert(0, str(center[1]))

    def update_data_source(self):
        """Update data source based on selected lake"""
        if not TKINTER_AVAILABLE:
            return
            
        lake = self.lake_selection.get().lower()
        source = f"gl_{lake}"
        
        if source in self.config.get('data_sources', {}):
            self.data_source.set(source)

    def browse_local(self):
        if not TKINTER_AVAILABLE:
            logger.error("Tkinter not available for file dialog")
            return
            
        file = filedialog.askopenfilename(
            title="Select Ocean Data File",
            filetypes=[("NetCDF", "*.nc"), ("All files", "*.*")]
        )
        if file:
            self.data_source.set('local')
            self.local_file_path = file
            self.data_status.config(text=f"Selected: {os.path.basename(file)}")

    def browse_sonar(self):
        if not TKINTER_AVAILABLE:
            logger.error("Tkinter not available for file dialog")
            return
            
        file = filedialog.askopenfilename(
            title="Select Sonar Data File",
            filetypes=[("RSD", "*.rsd"), ("NetCDF", "*.nc"), ("KML", "*.kml"), ("All files", "*.*")]
        )
        if file:
            self.sonar_entry.delete(0, tk.END)
            self.sonar_entry.insert(0, file)

    def update_data_sources(self):
        """Update all data sources and cache in database"""
        try:
            if TKINTER_AVAILABLE:
                result = messagebox.askyesno("Update Data", "This will download fresh data from all sources. Continue?")
                if not result:
                    return
                    
                self.data_status.config(text="Updating data sources...")
                self.root.update()
            
            # Run update in background thread
            import threading
            def update_worker():
                try:
                    auto_update_all_data()
                    if TKINTER_AVAILABLE:
                        self.root.after(0, lambda: self.data_status.config(text="Data sources updated successfully"))
                except Exception as e:
                    logger.error(f"Data update failed: {e}")
                    if TKINTER_AVAILABLE:
                        self.root.after(0, lambda: self.data_status.config(text=f"Data update failed: {e}"))
            
            update_thread = threading.Thread(target=update_worker, daemon=True)
            update_thread.start()
            
        except Exception as e:
            logger.error(f"Failed to start data update: {e}")
            if TKINTER_AVAILABLE:
                messagebox.showerror("Error", f"Failed to start data update: {e}")

    def load_data(self):
        try:
            messagebox.showinfo("Info", "Automatic data loading disabled. Please use 'Browse Local File' to load a NetCDF file manually.")
            return
        #.. rest of the method
        #try:
            if not OPENDRIFT_AVAILABLE:
                logger.error("OpenDrift not available for data loading")
                if TKINTER_AVAILABLE:
                    messagebox.showerror("Error", "OpenDrift not available")
                return
                
            source = self.data_source.get()
            if TKINTER_AVAILABLE:
                self.data_status.config(text=f"Loading {source} data...")
                self.root.update()
            
            if source == 'local':
                if not self.local_file_path:
                    if TKINTER_AVAILABLE:
                        messagebox.showerror("Error", "Please select a local file first")
                    return
                    
                if self.local_file_path.endswith('.kml'):
                    # Handle KML files for seeding locations
                    tree = ET.parse(self.local_file_path)
                    ns = {'kml': 'http://www.opengis.net/kml/2.2'}
                    points = []
                    for placemark in tree.findall('.//kml:Placemark', ns):
                        coords_elem = placemark.find('.//kml:Point/kml:coordinates', ns)
                        if coords_elem is not None:
                            coords = coords_elem.text.strip().split(',')
                            if len(coords) >= 2:
                                lat, lon = float(coords[1]), float(coords[0])
                                points.append((lat, lon))
                    
                    if points:
                        avg_lat = sum(p[0] for p in points) / len(points)
                        avg_lon = sum(p[1] for p in points) / len(points)
                        if TKINTER_AVAILABLE:
                            self.lat_entry.delete(0, tk.END)
                            self.lat_entry.insert(0, str(avg_lat))
                            self.lon_entry.delete(0, tk.END)
                            self.lon_entry.insert(0, str(avg_lon))
                    
                    if TKINTER_AVAILABLE:
                        self.data_status.config(text=f"Loaded {len(points)} points from KML")
                    return
                else:
                    # Load NetCDF file using safe method
                    try:
                        dataset = safe_open_netcdf(self.local_file_path)
                        self.reader = reader_netCDF_CF_generic.Reader(dataset)
                    except Exception as e:
                        logger.error(f"Failed to open NetCDF file: {e}")
                        raise
            else:
                # Load from ERDDAP server
                lake = source.replace('gl_', '')
                if TKINTER_AVAILABLE:
                    bbox = [
                        float(self.west.get()), 
                        float(self.east.get()), 
                        float(self.south.get()), 
                        float(self.north.get())
                    ]
                    
                    # Parse time range
                    start_time = parse(self.start_time.get())
                    end_time = parse(self.end_time.get())
                    time_range = [start_time, end_time]
                else:
                    # Use defaults for console mode
                    bbox = self.config['great_lakes_bbox'].get(lake, [-92.5, -76.0, 41.2, 49.5])
                    end_time = datetime.utcnow()
                    start_time = end_time - timedelta(hours=24)
                    time_range = [start_time, end_time]
                
                # Fetch data from ERDDAP
                data_file = fetch_gl_erddap_data(lake, time_range, bbox)
                
                if data_file:
                    # Use safe method to open the NetCDF file
                    try:
                        dataset = safe_open_netcdf(data_file)
                        self.reader = reader_netCDF_CF_generic.Reader(dataset)
                        self.current_lake = lake
                    except Exception as e:
                        logger.error(f"Failed to create reader from downloaded data: {e}")
                        raise
                else:
                    raise Exception("Failed to fetch data from ERDDAP")
            
            # Add landmask for better visualization
            try:
                self.landmask = reader_global_landmask.Reader()
            except Exception as e:
                logger.warning(f"Could not load landmask data: {e}")
                self.landmask = None
            
            if TKINTER_AVAILABLE:
                self.data_status.config(text=f"Data loaded successfully from {source}")
            logger.info(f"Data loaded from {source}")
            
        except Exception as e:
            logger.error(f"Data load failed: {e}")
            # Try to use cached data as fallback
            try:
                data_dir = self.config.get('directories', {}).get('data', 'data')
                if os.path.exists(data_dir):
                    cache_files = [f for f in os.listdir(data_dir) if f.endswith('.nc')]
                    if cache_files:
                        latest_file = max(cache_files, key=lambda x: os.path.getctime(os.path.join(data_dir, x)))
                        file_path = os.path.join(data_dir, latest_file)
                        # Use safe method to open the NetCDF file
                        dataset = safe_open_netcdf(file_path)
                        self.reader = reader_netCDF_CF_generic.Reader(dataset)
                        if TKINTER_AVAILABLE:
                            self.data_status.config(text=f"Using cached data: {latest_file}")
                        logger.info(f"Using cached data: {latest_file}")
                        return
                    else:
                        raise Exception("No cached data available")
                else:
                    raise Exception("Data directory does not exist")
            except Exception as e2:
                if TKINTER_AVAILABLE:
                    self.data_status.config(text="Data load failed - no cached data available")
                    messagebox.showerror("Error", f"Data load failed: {e}\nFallback also failed: {e2}")
                else:
                    logger.error(f"Data load failed: {e}\nFallback also failed: {e2}")

    def seed_particles(self):
        try:
            if not self.reader:
                if TKINTER_AVAILABLE:
                    messagebox.showerror("Error", "Please load ocean data first")
                else:
                    logger.error("Please load ocean data first")
                return
                
            if TKINTER_AVAILABLE:
                lon = float(self.lon_entry.get())
                lat = float(self.lat_entry.get())
                radius = float(self.radius_entry.get())
                count = int(self.count_entry.get())
            else:
                # Default values for console mode
                lon = -87.0
                lat = 43.8
                radius = 2.0
                count = 60
            
            # Initialize drift model if not already done
            if not self.drift_model:
                self.drift_model = EnhancedOceanDrift(loglevel=20)
                self.drift_model.add_reader(self.reader)
                
                # Add landmask if available
                if hasattr(self, 'landmask') and self.landmask:
                    self.drift_model.add_reader(self.landmask)
                
                # Configure for Great Lakes
                self.drift_model.set_config('drift:vertical_mixing', False)  # Great Lakes: shallow
                self.drift_model.set_config('drift:advection_scheme', 'euler')
                self.drift_model.set_config('drift:current_uncertainty', 0.05)
                self.drift_model.set_config('drift:wind_uncertainty', 0.1)
                # Remove tides config as it may not exist in all OpenDrift versions
                try:
                    self.drift_model.set_config('drift:tides', False)
                except:
                    logger.warning("Could not set tides configuration - continuing without it")
            
            # Get windage for selected object type
            try:
                conn = sqlite3.connect('drift_objects.db')
                if TKINTER_AVAILABLE:
                    obj_type = self.object_type.get()
                else:
                    obj_type = 'Vessel'
                
                if PANDAS_AVAILABLE:
                    windage_df = pd.read_sql_query(
                        f"SELECT windage FROM objects WHERE type='{obj_type}'", 
                        conn
                    )
                    if len(windage_df) > 0:
                        windage = windage_df.iloc[0]['windage']
                    else:
                        windage = 0.03  # default
                else:
                    c = conn.cursor()
                    c.execute(f"SELECT windage FROM objects WHERE type='{obj_type}'")
                    result = c.fetchone()
                    windage = result[0] if result else 0.03
                conn.close()
            except Exception as e:
                logger.warning(f"Could not get windage from database: {e}, using default")
                windage = 0.03
            
            self.drift_model.set_config('drift:wind_drift_factor', windage)
            
            # Parse seed start time
            if TKINTER_AVAILABLE:
                seed_time = parse(self.seed_start_time.get())
            else:
                seed_time = datetime.now()
            
            # Seed elements
            self.drift_model.seed_elements(
                lon=lon, 
                lat=lat, 
                radius=radius * 1852,  # Convert nm to meters
                number=count, 
                time=seed_time,
                wind_drift_factor=windage
            )
            
            if TKINTER_AVAILABLE:
                messagebox.showinfo("Success", f"Seeded {count} particles at ({lon}, {lat})")
            logger.info(f"Seeded {count} particles at ({lon}, {lat})")
            
        except Exception as e:
            logger.error(f"Particle seeding failed: {e}")
            if TKINTER_AVAILABLE:
                messagebox.showerror("Error", f"Particle seeding failed: {e}")

    def run_simulation(self):
        try:
            if not self.drift_model or not self.drift_model.num_elements_active():
                if TKINTER_AVAILABLE:
                    messagebox.showerror("Error", "Please seed particles first")
                else:
                    logger.error("Please seed particles first")
                return
                
            if TKINTER_AVAILABLE:
                duration = float(self.duration_entry.get())
                time_step = float(self.timestep_entry.get())
                backward = self.backward_var.get()
                use_ml = self.use_ml_var.get()
            else:
                # Default values for console mode
                duration = 24
                time_step = 10
                backward = False
                use_ml = False
            
            # Update ML model usage
            if use_ml:
                model_path = 'models/drift_correction.pkl'
                if os.path.exists(model_path):
                    self.drift_model.ml_model = joblib.load(model_path)
                else:
                    if TKINTER_AVAILABLE:
                        messagebox.showwarning("Warning", "No ML model found. Running without ML correction.")
                    else:
                        logger.warning("No ML model found. Running without ML correction.")
            
            # Run simulation in background thread
            if TKINTER_AVAILABLE:
                self.sim_status.config(text="Simulation running...")
                self.progress.start()
            
            if TKINTER_AVAILABLE:
                threading.Thread(
                    target=self._run_drift, 
                    args=(duration, time_step, backward),
                    daemon=True
                ).start()
            else:
                self._run_drift(duration, time_step, backward)
            
        except Exception as e:
            logger.error(f"Simulation setup failed: {e}")
            if TKINTER_AVAILABLE:
                messagebox.showerror("Error", f"Simulation setup failed: {e}")

    def _run_drift(self, duration, time_step, backward):
        try:
            # Convert hours to timedelta
            duration_td = timedelta(hours=duration)
            if backward:
                duration_td = -duration_td
                
            # Convert minutes to seconds
            time_step_sec = time_step * 60
            
            # Ensure outputs directory exists
            os.makedirs('outputs', exist_ok=True)
            
            # Run the simulation
            self.drift_model.run(
                duration=duration_td, 
                time_step=time_step_sec,
                time_step_output=time_step_sec * 4,  # Output every 4 time steps to reduce file size
                outfile='outputs/drift_simulation.nc'
            )
            
            # Export results
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            
            # Create plot with better visualization for Great Lakes
            self.create_enhanced_plot(timestamp)
            
            # Export to other formats
            try:
                self.drift_model.export_kml(f'outputs/drift_{timestamp}.kml')
            except Exception as e:
                logger.error(f"KML export failed: {e}")
            
            # Update UI in main thread
            if TKINTER_AVAILABLE:
                self.root.after(0, lambda: self.sim_status.config(text="Simulation complete!"))
                self.root.after(0, self.progress.stop)
                self.root.after(0, lambda: messagebox.showinfo(
                    "Success", 
                    f"Simulation complete. Results saved to outputs folder."
                ))
            
            logger.info(f"Simulation completed successfully. Results saved.")
            
        except Exception as e:
            logger.error(f"Simulation failed: {e}")
            if TKINTER_AVAILABLE:
                self.root.after(0, lambda: self.sim_status.config(text="Simulation failed!"))
                self.root.after(0, self.progress.stop)
                self.root.after(0, lambda: messagebox.showerror("Error", f"Simulation failed: {e}"))


def overlay_sonar(self):
    try:
        if not TKINTER_AVAILABLE:
            logger.error("Sonar overlay requires GUI mode")
            return
            
        sonar_file = self.sonar_entry.get()
        if not sonar_file:
            messagebox.showerror("Error", "Please select a sonar file first")
            return
            
        messagebox.showinfo("Info", "Sonar overlay functionality not fully implemented yet")
        
    except Exception as e:
        logger.error(f"Sonar overlay failed: {e}")
        messagebox.showerror("Error", f"Sonar overlay failed: {e}")


if __name__ == "__main__":
    try:
        if TKINTER_AVAILABLE:
            print("Creating GUI window...")
            root = tk.Tk()
            root.lift()  # Bring to front
            root.attributes('-topmost', True)  # Force on top
            root.after_idle(root.attributes, '-topmost', False)  # Remove topmost after appearing
            
            print("Initializing CESAROPS application...")
            app = SAROpsApp(root)
            
            print("Starting main loop...")
            root.mainloop()
            print("GUI closed.")
        else:
            print("Tkinter not available, starting console mode...")
            run_cli_mode()
    except Exception as e:
        print(f"Error starting application: {e}")
        import traceback
        traceback.print_exc()
        input("Press Enter to exit...")
